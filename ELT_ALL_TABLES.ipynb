{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The correct one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apscheduler.schedulers.background import BackgroundScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Connection details for source and target databases\n",
    "server_source = 'MENNAS-LAPTOP\\\\MSSQLSERVER22'\n",
    "database_source = 'AdventureWorks2019'\n",
    "username_source = 'sa'\n",
    "password_source = 'sa123456'\n",
    "driver = 'ODBC Driver 17 for SQL Server'\n",
    "\n",
    "server_target = 'MENNAS-LAPTOP\\\\MSSQLSERVER22'\n",
    "database_target = 'HR'\n",
    "username_target = 'sa'\n",
    "password_target = 'sa123456'\n",
    "\n",
    "# Create engine for source and target databases\n",
    "connection_string_source = (\n",
    "    f'mssql+pyodbc://{username_source}:{password_source}@{server_source}/{database_source}'\n",
    "    f'?driver={driver.replace(\" \", \"+\")}'\n",
    ")\n",
    "engine_source = create_engine(connection_string_source, echo=False)\n",
    "\n",
    "connection_string_target = (\n",
    "    f'mssql+pyodbc://{username_target}:{password_target}@{server_target}/{database_target}'\n",
    "    f'?driver={driver.replace(\" \", \"+\")}'\n",
    ")\n",
    "engine_target = create_engine(connection_string_target, echo=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import traceback\n",
    "import logging\n",
    "from datetime import date,timedelta,datetime\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import getpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def send_mail_error(\n",
    "    sender=\"osamamenna1511@gmail.com\",  # put sender email here \n",
    "    password='lwrx jrtd uhjm cjvt',  # add a comma here\n",
    "    to_list=[\"menna1eminshawy@gmail.com\"],  # receiver list here\n",
    "    error=None,\n",
    "    subject=\"Error AD\",\n",
    "):\n",
    "    msg = MIMEMultipart()\n",
    "    msg[\"From\"] = sender\n",
    "    msg[\"To\"] = \", \".join(to_list)\n",
    "    msg[\"Subject\"] = subject\n",
    "    message = f\"\"\"<html><body>{error}</body></html>\"\"\"\n",
    "    msg.attach(MIMEText(message, \"html\"))\n",
    "    \n",
    "    # sends email\n",
    "    smtpserver = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
    "    smtpserver.starttls()\n",
    "    smtpserver.login(sender, password)  # use the 'sender' variable instead of hardcoding\n",
    "    smtpserver.sendmail(msg[\"From\"], to_list, msg.as_string())\n",
    "    smtpserver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success email sent successfully.\n"
     ]
    }
   ],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def send_mail_success(\n",
    "    sender=\"osamamenna1511@gmail.com\",  # put sender email here \n",
    "    password='lwrx jrtd uhjm cjvt',  # use your app-specific password here\n",
    "    to_list=[\"menna1eminshawy@gmail.com\"],  # receiver list here\n",
    "    message=\"Data transformation completed successfully.\",\n",
    "    subject=\"Data Transformation Success\",\n",
    "):\n",
    "    msg = MIMEMultipart()\n",
    "    msg[\"From\"] = sender\n",
    "    msg[\"To\"] = \", \".join(to_list)\n",
    "    msg[\"Subject\"] = subject\n",
    "    html_message = f\"\"\"<html><body>{message}</body></html>\"\"\"\n",
    "    msg.attach(MIMEText(html_message, \"html\"))\n",
    "    \n",
    "    try:\n",
    "        # sends email using IP address as a temporary workaround\n",
    "        smtpserver = smtplib.SMTP(\"74.125.200.108\", 587)  # Gmail SMTP IP address\n",
    "        smtpserver.starttls()\n",
    "        smtpserver.login(sender, password)\n",
    "        smtpserver.sendmail(msg[\"From\"], to_list, msg.as_string())\n",
    "        smtpserver.quit()\n",
    "        print(\"Success email sent successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send success email: {e}\")\n",
    "\n",
    "# Example usage after data transformation is done\n",
    "send_mail_success(\n",
    "    message=\"The data transformation process has been completed successfully and is ready for further steps.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_foreign_keys(engine):\n",
    "    with engine.connect() as connection:\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            ss.name AS SchemaName,\n",
    "            tp.name AS TableName\n",
    "        FROM \n",
    "    sys.foreign_keys AS fk\n",
    "JOIN \n",
    "    sys.foreign_key_columns AS fkc \n",
    "    ON fk.object_id = fkc.constraint_object_id\n",
    "JOIN \n",
    "    sys.tables AS tp \n",
    "    ON fkc.parent_object_id = tp.object_id\n",
    "JOIN \n",
    "    sys.columns AS cp \n",
    "    ON fkc.parent_object_id = cp.object_id AND fkc.parent_column_id = cp.column_id\n",
    "JOIN \n",
    "    sys.tables AS tr \n",
    "    ON fkc.referenced_object_id = tr.object_id\n",
    "JOIN \n",
    "    sys.columns AS cr \n",
    "    ON fkc.referenced_object_id = cr.object_id AND fkc.referenced_column_id = cr.column_id\n",
    "join sys.schemas ss on ss.schema_id = fk.schema_id\n",
    "        \"\"\"\n",
    "        result = connection.execute(text(query)).fetchall()\n",
    "        return result  # Return the result for further use\n",
    "\n",
    "# Call the function to get the schema and table names\n",
    "# result = disable_foreign_keys(engine_target)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nocheck_constraints(engine, result):\n",
    "    for schema_name, table_name in result:\n",
    "        try:\n",
    "            with engine.connect() as connection:\n",
    "                # Set autocommit to True if needed; otherwise, remove this line\n",
    "                #connection = connection.execution_options(autocommit=True)\n",
    "                query = f'ALTER TABLE {schema_name}.{table_name} NOCHECK CONSTRAINT ALL'\n",
    "                # Log the query for debugging purposes\n",
    "                print(f\"Executing query: {query}\")\n",
    "                connection.execute(text(query).execution_options(autocommit=True)) ##change\n",
    "                print(f\"Constraints disabled for {schema_name}.{table_name}\")\n",
    "        except Exception as e:\n",
    "            # Print the error for debugging\n",
    "            print(f\"Error disabling constraints for {schema_name}.{table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nocheck_constraints(engine_target, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query: ALTER TABLE fact.Employee NOCHECK CONSTRAINT ALL\n",
      "Constraints disabled for fact.Employee\n",
      "Executing query: ALTER TABLE fact.Employee NOCHECK CONSTRAINT ALL\n",
      "Constraints disabled for fact.Employee\n",
      "Executing query: ALTER TABLE dimension.EmployeeDepartmentHistory NOCHECK CONSTRAINT ALL\n",
      "Constraints disabled for dimension.EmployeeDepartmentHistory\n",
      "Executing query: ALTER TABLE dimension.EmployeeDepartmentHistory NOCHECK CONSTRAINT ALL\n",
      "Constraints disabled for dimension.EmployeeDepartmentHistory\n",
      "Executing query: ALTER TABLE dimension.Employee_Payment NOCHECK CONSTRAINT ALL\n",
      "Constraints disabled for dimension.Employee_Payment\n",
      "Executing query: ALTER TABLE dimension.Personal_information NOCHECK CONSTRAINT ALL\n",
      "Constraints disabled for dimension.Personal_information\n",
      "DELETE FROM dimension.Department\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy.orm import sessionmaker\n",
    "def delete_table_contents(engine, schema, table_name):\n",
    "    session = sessionmaker(engine_target)\n",
    "    with session.begin() as sess:\n",
    "        delete_statement = f'DELETE FROM {schema}.{table_name}'\n",
    "        print(delete_statement)\n",
    "        sess.execute(text(delete_statement))\n",
    "result = disable_foreign_keys(engine_target)\n",
    "nocheck_constraints(engine_target, result)\n",
    "delete_table_contents(engine_target, 'dimension', 'Department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def delete_table_contents(engine, schema, table_name):\n",
    "#     with engine.connect() as connection:\n",
    "#         delete_statement = f'DELETE FROM {schema}.{table_name}'\n",
    "#         print(delete_statement)\n",
    "#         connection.execute(text(delete_statement).execution_options(autocommit=True))\n",
    "#     print(f'Deleted contents of table {schema}.{table_name}')\n",
    "# # result = disable_foreign_keys(engine_target)\n",
    "# # nocheck_constraints(engine_target, result)\n",
    "\n",
    "# # Delete contents from the 'dimension.shift' table\n",
    "# # delete_table_contents(engine_target, 'dimension', 'shift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query: ALTER TABLE fact.Employee NOCHECK CONSTRAINT ALL\n",
      "Constraints disabled for fact.Employee\n",
      "Executing query: ALTER TABLE fact.Employee NOCHECK CONSTRAINT ALL\n",
      "Constraints disabled for fact.Employee\n",
      "Executing query: ALTER TABLE dimension.EmployeeDepartmentHistory NOCHECK CONSTRAINT ALL\n",
      "Constraints disabled for dimension.EmployeeDepartmentHistory\n",
      "Executing query: ALTER TABLE dimension.EmployeeDepartmentHistory NOCHECK CONSTRAINT ALL\n",
      "Constraints disabled for dimension.EmployeeDepartmentHistory\n",
      "Executing query: ALTER TABLE dimension.Employee_Payment NOCHECK CONSTRAINT ALL\n",
      "Constraints disabled for dimension.Employee_Payment\n",
      "Executing query: ALTER TABLE dimension.Personal_information NOCHECK CONSTRAINT ALL\n",
      "Constraints disabled for dimension.Personal_information\n",
      "DELETE FROM dimension.Department\n"
     ]
    }
   ],
   "source": [
    "result = disable_foreign_keys(engine_target)\n",
    "nocheck_constraints(engine_target, result)\n",
    "delete_table_contents(engine_target, 'dimension', 'Department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table dimension.department is empty.\n"
     ]
    }
   ],
   "source": [
    "def check_table_empty(engine, schema, table_name):\n",
    "    with engine.connect() as connection:\n",
    "        query = f'SELECT COUNT(*) FROM {schema}.{table_name}'\n",
    "        result = connection.execute(text(query)).scalar()\n",
    "    if result == 0:\n",
    "        print(f'The table {schema}.{table_name} is empty.')\n",
    "    else:\n",
    "        print(f'The table {schema}.{table_name} has {result} rows.')\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "# delete_table_contents(engine_target, 'dimension', 'department')\n",
    "check_table_empty(engine_target, 'dimension', 'department')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and load tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from Employee loaded into DataFrame:\n",
      "DELETE FROM dimension.employeelogin\n"
     ]
    }
   ],
   "source": [
    "def extract_and_load_employeelogin():\n",
    "    query = \"SELECT BusinessEntityID, LoginID, rowguid FROM HumanResources.Employee\"\n",
    "    df = pd.read_sql(query, engine_source)\n",
    "    print(\"Data from Employee loaded into DataFrame:\")\n",
    "   \n",
    "    delete_table_contents(engine_target, 'dimension', 'employeelogin')\n",
    "    df.to_sql(name='Employeelogin', con=engine_target, schema='dimension',if_exists = 'append', index=False)\n",
    "\n",
    "    \n",
    "extract_and_load_employeelogin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Job (id=b4180a5fef9a4df2b36cd03b8cea5f78 name=extract_and_load_employeelogin)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of BackgroundScheduler\n",
    "scheduler = BackgroundScheduler()\n",
    "scheduler.start()\n",
    "scheduler.add_job(func=extract_and_load_employeelogin, trigger=\"interval\", minutes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from Department loaded into DataFrame:\n",
      "DELETE FROM dimension.Department\n",
      "Done!\n",
      "Success email sent successfully.\n"
     ]
    }
   ],
   "source": [
    "def extract_and_load_department():\n",
    "    query = \"SELECT DISTINCT DepartmentID, Name, GroupName, ModifiedDate FROM HumanResources.Department\"\n",
    "    df = pd.read_sql(query, engine_source)\n",
    "    print(\"Data from Department loaded into DataFrame:\")\n",
    "    \n",
    "  \n",
    "    delete_table_contents(engine_target, 'dimension', 'Department')\n",
    "    \n",
    "    df.to_sql(name='Department', con=engine_target, schema='dimension', if_exists='append', index=False)\n",
    "    \n",
    "   \n",
    "    print('Done!')\n",
    "    \n",
    "    send_mail_success(\n",
    "    subject='ETL Success: Department Transformation',\n",
    "    message='The data transformation for the Department was successful.'\n",
    "     )\n",
    "\n",
    "# Execute the extract and load function\n",
    "extract_and_load_department()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Job (id=f426577f8ac94cfcac8b650d06506943 name=extract_and_load_department)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.add_job(func=extract_and_load_department, trigger=\"interval\", minutes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Employee Data:\n",
      "   BusinessEntityID\n",
      "0               263\n",
      "1                78\n",
      "2               242\n",
      "3               125\n",
      "4               278\n",
      "Extracted EmployeeDepartmentHistory Data:\n",
      "   BusinessEntityID  DepartmentID\n",
      "0                 2             1\n",
      "1                 3             1\n",
      "2                 3             2\n",
      "3                 4             1\n",
      "4                 4             2\n",
      "Merged DataFrame:\n",
      "   BusinessEntityID  DepartmentID\n",
      "0               263            11\n",
      "1                78             7\n",
      "2               242            10\n",
      "3               125            15\n",
      "4               278             3\n",
      "DataFrame after Removing Duplicates:\n",
      "   BusinessEntityID  DepartmentID\n",
      "0               263            11\n",
      "1                78             7\n",
      "2               242            10\n",
      "3               125            15\n",
      "4               278             3\n",
      "Final Employee Fact DataFrame:\n",
      "   BusinessEntityID  DepartmentID\n",
      "0               263            11\n",
      "1                78             7\n",
      "2               242            10\n",
      "3               125            15\n",
      "4               278             3\n",
      "DELETE FROM fact.Employee\n",
      "Data successfully inserted into fact.Employee\n",
      "Success email sent successfully.\n"
     ]
    }
   ],
   "source": [
    "def extract_load_employee_fact():\n",
    "    try:\n",
    "        # Extract Employee data\n",
    "        df_employee = pd.read_sql(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                BusinessEntityID\n",
    "            FROM HumanResources.Employee\n",
    "            \"\"\",\n",
    "            engine_source\n",
    "        )\n",
    "        print(\"Extracted Employee Data:\")\n",
    "        print(df_employee.head())  # Print the first few rows\n",
    "\n",
    "        # Extract EmployeeDepartmentHistory data\n",
    "        df_employee_dept_history = pd.read_sql(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                BusinessEntityID,\n",
    "                DepartmentID\n",
    "            FROM HumanResources.EmployeeDepartmentHistory\n",
    "            \"\"\",\n",
    "            engine_source\n",
    "        )\n",
    "        print(\"Extracted EmployeeDepartmentHistory Data:\")\n",
    "        print(df_employee_dept_history.head())  # Print the first few rows\n",
    "\n",
    "        # Merge data\n",
    "        df_employee_fact = pd.merge(\n",
    "            df_employee,\n",
    "            df_employee_dept_history,\n",
    "            on='BusinessEntityID',\n",
    "            how='left'\n",
    "        )\n",
    "        print(\"Merged DataFrame:\")\n",
    "        print(df_employee_fact.head())  # Print the first few rows\n",
    "\n",
    "        # Remove duplicates based on BusinessEntityID\n",
    "        df_employee_fact = df_employee_fact.drop_duplicates(subset=['BusinessEntityID'])\n",
    "        print(\"DataFrame after Removing Duplicates:\")\n",
    "        print(df_employee_fact.head())  # Print the first few rows\n",
    "\n",
    "        # We do not need to handle EmployeeID, as it will be auto-generated by SQL Server\n",
    "        df_employee_fact = df_employee_fact[['BusinessEntityID', 'DepartmentID']]\n",
    "\n",
    "        # Print DataFrame for verification\n",
    "        print(\"Final Employee Fact DataFrame:\")\n",
    "        print(df_employee_fact.head())  # Print the first few rows\n",
    "\n",
    "        # Load data to target table\n",
    "        target_schema = 'fact'\n",
    "        target_table = 'Employee'\n",
    "\n",
    "        # Delete existing contents of the fact.Employee table\n",
    "        delete_table_contents(engine_target, target_schema, target_table)\n",
    "\n",
    "        # Insert new data into the fact.Employee table\n",
    "        df_employee_fact.to_sql(name=target_table, con=engine_target, schema=target_schema, if_exists='append', index=False, chunksize=1000)\n",
    "        print(f\"Data successfully inserted into {target_schema}.{target_table}\")\n",
    "        \n",
    "                \n",
    "        send_mail_success(\n",
    "        subject='ETL Success: Load Employee Fact Transformation',\n",
    "        message='The data transformation for the Load Employee Fact table was successful.'\n",
    "        )\n",
    "    except:\n",
    "        logger.exception(\"error test\")\n",
    "        send_mail_error(error = str (traceback.format_exc()),\n",
    "                    subject = 'Error test')\n",
    "\n",
    "# Run the ETL process for Employee Fact table\n",
    "extract_load_employee_fact()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Job (id=719942ffbc8f451880cc196e2128f9ee name=extract_load_employee_fact)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.add_job(func=extract_load_employee_fact, trigger=\"interval\", minutes=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from Shift loaded into DataFrame:\n",
      "   ShiftID     Name StartTime   EndTime ModifiedDate\n",
      "0        1      Day  07:00:00  15:00:00   2008-04-30\n",
      "1        2  Evening  15:00:00  23:00:00   2008-04-30\n",
      "2        3    Night  23:00:00  07:00:00   2008-04-30\n",
      "DELETE FROM dimension.shift\n",
      "Data successfully inserted into dimension.shift\n",
      "Success email sent successfully.\n",
      "Constraints re-enabled.\n"
     ]
    }
   ],
   "source": [
    "def extract_and_load_shift():\n",
    "    query = \"SELECT DISTINCT ShiftID, Name, StartTime, EndTime, ModifiedDate FROM HumanResources.Shift\"\n",
    "    df = pd.read_sql(query, engine_source)\n",
    "    print(f\"Data from Shift loaded into DataFrame:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        \n",
    "        # Delete existing records in the target table\n",
    "        delete_table_contents(engine_target, 'dimension', 'shift')\n",
    "        \n",
    "        # Load new data into the target table\n",
    "        df.to_sql(name='shift', con=engine_target, schema='dimension', if_exists='append', index=False)\n",
    "        print('Data successfully inserted into dimension.shift')\n",
    "        \n",
    "        send_mail_success(\n",
    "        subject='ETL Success:Shift Transformation',\n",
    "        message='The data transformation for the Shift table was successful.'\n",
    "        )\n",
    "    except:\n",
    "        logger.exception(\"error test\")\n",
    "        send_mail_error(error = str (traceback.format_exc()),\n",
    "                    subject = 'Error test')  \n",
    "\n",
    "    finally:\n",
    "        # Enable constraints\n",
    "       \n",
    "        print('Constraints re-enabled.')\n",
    "\n",
    "# Run the ETL process for Shift table\n",
    "extract_and_load_shift()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Job (id=3ac84fd1b5434c5b8c33ce91c983354d name=extract_and_load_shift)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scheduler.add_job(func=extract_and_load_shift, trigger=\"interval\", minutes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully updated and inserted into dimension.EmployeeDepartmentHistory\n",
      "Success email sent successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "\n",
    "def timestamp_to_int(ts):\n",
    "    \"\"\"Convert Timestamp to an integer format.\"\"\"\n",
    "    if pd.isnull(ts):\n",
    "        return None\n",
    "    return int.from_bytes(ts.to_pydatetime().timestamp().to_bytes(8, byteorder='little', signed=False), byteorder='big')\n",
    "\n",
    "def extract_merge_load_employee_department_history():\n",
    "    with engine_target.connect() as conn:\n",
    "        # Fetch existing records\n",
    "        existing_records = pd.read_sql_query(\"\"\"\n",
    "            SELECT BusinessEntityID, ShiftID\n",
    "            FROM dimension.EmployeeDepartmentHistory\n",
    "            \"\"\", conn)\n",
    "        \n",
    "        # Convert existing records to a set for quick lookup\n",
    "        existing_records_set = set(existing_records.itertuples(index=False, name=None))\n",
    "        \n",
    "        # Placeholder for loading your new data (replace 'your_dataframe' with actual DataFrame)\n",
    "        # Example: your_dataframe = pd.read_sql_query(\"SELECT * FROM source_table\", engine_source)\n",
    "        # Ensure that 'your_dataframe' is defined before this\n",
    "        your_dataframe = pd.DataFrame()  # Replace with actual code to load new data\n",
    "        \n",
    "        # Iterate through your new records\n",
    "        for _, row in your_dataframe.iterrows():\n",
    "            business_entity_id = row['BusinessEntityID']\n",
    "            shift_id = row['ShiftID']\n",
    "            start_date = row['StartDate']\n",
    "            modified_date = row['ModifiedDate']\n",
    "            job_title = row['JobTitle']\n",
    "            \n",
    "            # Check for existing record\n",
    "            if (business_entity_id, shift_id) in existing_records_set:\n",
    "                print(f\"Record with BusinessEntityID={business_entity_id} and ShiftID={shift_id} already exists.\")\n",
    "            else:\n",
    "                # Insert new record\n",
    "                insert_query = text(\"\"\"\n",
    "                    INSERT INTO dimension.EmployeeDepartmentHistory \n",
    "                    (BusinessEntityID, ShiftID, StartDate, EndDate, ModifiedDate, JobTitle, IsCurrent)\n",
    "                    VALUES (:business_entity_id, :shift_id, :start_date, NULL, :modified_date, :job_title, 1)\n",
    "                \"\"\")\n",
    "                conn.execute(insert_query, {\n",
    "                    'business_entity_id': business_entity_id,\n",
    "                    'shift_id': shift_id,\n",
    "                    'start_date': start_date,\n",
    "                    'modified_date': modified_date,\n",
    "                    'job_title': job_title\n",
    "                })\n",
    "        print(\"Data successfully updated and inserted into dimension.EmployeeDepartmentHistory\")\n",
    "        \n",
    "        send_mail_success(\n",
    "        subject='ETL Success: Employee Department History Transformation',\n",
    "        message='The data transformation for theEmployee Department History table was successful.'\n",
    "        )\n",
    "\n",
    "# Call your function\n",
    "extract_merge_load_employee_department_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Job (id=18ee633a39744f65946c52405866545e name=extract_and_load_shift)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scheduler.add_job(func=extract_and_load_shift, trigger=\"interval\", minutes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmployeePayHistory data loaded into DataFrame:\n",
      "   BusinessEntityID RateChangeDate  Salary_hourly_rate  PayFrequency  \\\n",
      "0                 1     2009-01-14            125.5000             1   \n",
      "1                 2     2008-01-31             63.4615             2   \n",
      "2                 3     2007-11-11             43.2692             2   \n",
      "3                 4     2007-12-05              8.6200             2   \n",
      "4                 4     2010-05-31             23.7200             2   \n",
      "\n",
      "  ModifiedDate  \n",
      "0   2014-06-30  \n",
      "1   2014-06-30  \n",
      "2   2014-06-30  \n",
      "3   2007-11-21  \n",
      "4   2010-05-16  \n",
      "Employee data loaded into DataFrame:\n",
      "   BusinessEntityID  CurrentFlag  SalariedFlag\n",
      "0                 1         True          True\n",
      "1                 2         True          True\n",
      "2                 3         True          True\n",
      "3                 4         True         False\n",
      "4                 5         True          True\n",
      "Transformed DataFrame:\n",
      "   BusinessEntityID RateChangeDate  Salary_hourly_rate  \\\n",
      "0                 1     2009-01-14            125.5000   \n",
      "1                 2     2008-01-31             63.4615   \n",
      "2                 3     2007-11-11             43.2692   \n",
      "3                 4     2007-12-05              8.6200   \n",
      "4                 4     2010-05-31             23.7200   \n",
      "\n",
      "   Salary_received_monthly  Salary_received_weekly ModifiedDate Inactive  \\\n",
      "0                      1.0                     NaN   2014-06-30     None   \n",
      "1                      NaN                     1.0   2014-06-30     None   \n",
      "2                      NaN                     1.0   2014-06-30     None   \n",
      "3                      NaN                     1.0   2007-11-21     None   \n",
      "4                      NaN                     1.0   2010-05-16     None   \n",
      "\n",
      "   Active  Hourly  Salaried  \n",
      "0       1     NaN       1.0  \n",
      "1       1     NaN       1.0  \n",
      "2       1     NaN       1.0  \n",
      "3       1     1.0       NaN  \n",
      "4       1     1.0       NaN  \n",
      "DELETE FROM dimension.Employee_Payment\n",
      "The table dimension.Employee_Payment is empty.\n",
      "Data successfully inserted into dimension.Employee_Payment\n",
      "Success email sent successfully.\n"
     ]
    }
   ],
   "source": [
    "def extract_transform_load_employee_payment():\n",
    "    try:\n",
    "        # Extract EmployeePayHistory data\n",
    "        df_employee_pay_history = pd.read_sql(\n",
    "            \"SELECT BusinessEntityID, RateChangeDate, Rate AS Salary_hourly_rate, PayFrequency, ModifiedDate FROM HumanResources.EmployeePayHistory\",\n",
    "            engine_source\n",
    "        )\n",
    "        print(\"EmployeePayHistory data loaded into DataFrame:\")\n",
    "        print(df_employee_pay_history.head())\n",
    "\n",
    "        # Extract Employee data\n",
    "        df_employee = pd.read_sql(\n",
    "            \"SELECT BusinessEntityID, CurrentFlag, SalariedFlag FROM HumanResources.Employee\",\n",
    "            engine_source\n",
    "        )\n",
    "        print(\"Employee data loaded into DataFrame:\")\n",
    "        print(df_employee.head())\n",
    "\n",
    "        # Transform data\n",
    "        df_employee_pay_history['Salary_received_weekly'] = df_employee_pay_history['PayFrequency'].apply(lambda x: 1 if x == 2 else None)\n",
    "        df_employee_pay_history['Salary_received_monthly'] = df_employee_pay_history['PayFrequency'].apply(lambda x: 1 if x == 1 else None)\n",
    "\n",
    "        df_merged = pd.merge(\n",
    "            df_employee_pay_history.drop(columns=['PayFrequency']),\n",
    "            df_employee,\n",
    "            on='BusinessEntityID'\n",
    "        )\n",
    "\n",
    "        df_merged['Inactive'] = df_merged['CurrentFlag'].apply(lambda x: 1 if x == 0 else None)\n",
    "        df_merged['Active'] = df_merged['CurrentFlag'].apply(lambda x: 1 if x == 1 else None)\n",
    "        df_merged['Hourly'] = df_merged['SalariedFlag'].apply(lambda x: 1 if x == 0 else None)\n",
    "        df_merged['Salaried'] = df_merged['SalariedFlag'].apply(lambda x: 1 if x == 1 else None)\n",
    "\n",
    "        df_final = df_merged[[\n",
    "            'BusinessEntityID',\n",
    "            'RateChangeDate',\n",
    "            'Salary_hourly_rate',\n",
    "            'Salary_received_monthly',\n",
    "            'Salary_received_weekly',\n",
    "            'ModifiedDate',\n",
    "            'Inactive',\n",
    "            'Active',\n",
    "            'Hourly',\n",
    "            'Salaried'\n",
    "        ]]\n",
    "        print(\"Transformed DataFrame:\")\n",
    "        print(df_final.head())\n",
    "\n",
    "        # Load data to target table\n",
    "        target_schema = 'dimension'\n",
    "        target_table = 'Employee_Payment'\n",
    "\n",
    "     \n",
    "        delete_table_contents(engine_target, target_schema, target_table)\n",
    "        check_table_empty(engine_target, target_schema, target_table)\n",
    "\n",
    "        df_final.to_sql(name=target_table, con=engine_target, schema=target_schema, if_exists='append', index=False, chunksize=1000)\n",
    "        print(\"Data successfully inserted into dimension.Employee_Payment\")\n",
    "        \n",
    "        # Send email after successful transformation\n",
    "        send_mail_success(\n",
    "            subject='ETL Success: Employee Payment Transformation',\n",
    "            message='The data transformation for the Employee Payment table was successful.'\n",
    "        )\n",
    "\n",
    "    except:\n",
    "        logger.exception(\"error test\")\n",
    "        send_mail_error(error = str (traceback.format_exc()),\n",
    "                    subject = 'Error test')\n",
    "\n",
    "  \n",
    "\n",
    "# Run the ETL process for Employee_Payment table\n",
    "extract_transform_load_employee_payment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Job (id=27f6d4941d8f493fa2d3054e48f37eb6 name=extract_transform_load_employee_payment)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.add_job(func=extract_transform_load_employee_payment, trigger=\"interval\", minutes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee data loaded into DataFrame:\n",
      "   BusinessEntityID                       JobTitle\n",
      "0                 1        Chief Executive Officer\n",
      "1                 2  Vice President of Engineering\n",
      "2                 3            Engineering Manager\n",
      "3                 4           Senior Tool Designer\n",
      "4                 5                Design Engineer\n",
      "EmployeeDepartmentHistory data loaded into DataFrame:\n",
      "   BusinessEntityID  ShiftID   StartDate     EndDate            ModifiedDate\n",
      "0                 1        2  2009-01-14  2024-08-17 2024-08-17 10:36:30.557\n",
      "1                 1        2  2024-08-12  2024-08-17 2024-08-17 10:36:30.557\n",
      "2                 1        2  2024-08-12  2024-08-17 2024-08-17 10:36:30.557\n",
      "3                 2        1  2008-01-31        None 2008-01-30 00:00:00.000\n",
      "4                 2        2  2020-08-12        None 2024-08-12 13:38:03.163\n",
      "Merged DataFrame:\n",
      "   BusinessEntityID  ShiftID   StartDate     EndDate            ModifiedDate  \\\n",
      "0                 1        2  2009-01-14  2024-08-17 2024-08-17 10:36:30.557   \n",
      "1                 1        2  2024-08-12  2024-08-17 2024-08-17 10:36:30.557   \n",
      "2                 1        2  2024-08-12  2024-08-17 2024-08-17 10:36:30.557   \n",
      "3                 2        1  2008-01-31        None 2008-01-30 00:00:00.000   \n",
      "4                 2        2  2020-08-12        None 2024-08-12 13:38:03.163   \n",
      "\n",
      "                        JobTitle  \n",
      "0        Chief Executive Officer  \n",
      "1        Chief Executive Officer  \n",
      "2        Chief Executive Officer  \n",
      "3  Vice President of Engineering  \n",
      "4  Vice President of Engineering  \n",
      "DELETE FROM dimension.EmployeeDepartmentHistory\n",
      "The table dimension.EmployeeDepartmentHistory is empty.\n",
      "Data successfully inserted into dimension.EmployeeDepartmentHistory\n",
      "Success email sent successfully.\n"
     ]
    }
   ],
   "source": [
    "def extract_merge_load_employee_department_history():\n",
    "    # Extract JobTitle and BusinessEntityID from Employee table\n",
    "    query_employee = \"\"\"\n",
    "    SELECT BusinessEntityID, JobTitle\n",
    "    FROM HumanResources.Employee\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_employee = pd.read_sql(query_employee, engine_source)\n",
    "        print(\"Employee data loaded into DataFrame:\")\n",
    "        print(df_employee.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading employee data: {e}\")\n",
    "        return\n",
    "\n",
    "    # Extract EmployeeDepartmentHistory data\n",
    "    query_history = \"\"\"\n",
    "    SELECT BusinessEntityID, ShiftID, StartDate, EndDate, ModifiedDate\n",
    "    FROM HumanResources.EmployeeDepartmentHistory\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_history = pd.read_sql(query_history, engine_source)\n",
    "        print(\"EmployeeDepartmentHistory data loaded into DataFrame:\")\n",
    "        print(df_history.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading employee department history data: {e}\")\n",
    "        return\n",
    "\n",
    "    # Merge DataFrames on BusinessEntityID\n",
    "    df_merged = pd.merge(df_history, df_employee, on='BusinessEntityID', how='left')\n",
    "\n",
    "    print(\"Merged DataFrame:\")\n",
    "    print(df_merged.head())\n",
    "\n",
    "    # Load data to target table\n",
    "    target_schema = 'dimension'\n",
    "    target_table = 'EmployeeDepartmentHistory'\n",
    "\n",
    "    try:\n",
    "        delete_table_contents(engine_target, target_schema, target_table)\n",
    "        check_table_empty(engine_target, target_schema, target_table)\n",
    "\n",
    "        df_merged.to_sql(target_table, con=engine_target, schema=target_schema, if_exists='append', index=False, chunksize=1000)\n",
    "        print(\"Data successfully inserted into dimension.EmployeeDepartmentHistory\")\n",
    "        send_mail_success(\n",
    "        subject='ETL Success: Employee Department History Transformation',\n",
    "        message='The data transformation for the Employee Department History table was successful.'\n",
    "        )\n",
    "\n",
    "    except:\n",
    "        logger.exception(\"error test\")\n",
    "        send_mail_error(error = str (traceback.format_exc()),\n",
    "                    subject = 'Error test')\n",
    "\n",
    "      \n",
    "\n",
    "# Run the ETL process for EmployeeDepartmentHistory table\n",
    "extract_merge_load_employee_department_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Job (id=9e51fd438ee74b1391ad2a603ffe2b2a name=extract_merge_load_employee_department_history)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.add_job(func=extract_merge_load_employee_department_history, trigger=\"interval\", minutes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee information loaded into DataFrame:\n",
      "   BusinessEntityID NationalIDNumber   BirthDate MaritalStatus Gender  \\\n",
      "0                 1        295847284  1969-01-29             S      M   \n",
      "1                 2        245797967  1971-08-01             S      F   \n",
      "2                 3        509647174  1974-11-12             M      M   \n",
      "3                 4        112457891  1974-12-23             S      M   \n",
      "4                 5        695256908  1952-09-27             M      F   \n",
      "\n",
      "     HireDate  VacationHours  SickLeaveHours  \\\n",
      "0  2009-01-14             99              69   \n",
      "1  2008-01-31              1              20   \n",
      "2  2007-11-11              2              21   \n",
      "3  2007-12-05             48              80   \n",
      "4  2008-01-06              5              22   \n",
      "\n",
      "  employee_location_in_corporate_hierarchy  \\\n",
      "0                                     None   \n",
      "1                                      /1/   \n",
      "2                                    /1/1/   \n",
      "3                                  /1/1/1/   \n",
      "4                                  /1/1/2/   \n",
      "\n",
      "  employee_depth_in_corporate_hierarchy  \n",
      "0                                  None  \n",
      "1                                     1  \n",
      "2                                     2  \n",
      "3                                     3  \n",
      "4                                     3  \n",
      "JobCandidate information loaded into DataFrame:\n",
      "   BusinessEntityID                                             Resume\n",
      "0               NaN  <ns:Resume xmlns:ns=\"http://schemas.microsoft....\n",
      "1               NaN  <ns:Resume xmlns:ns=\"http://schemas.microsoft....\n",
      "2               NaN  <ns:Resume xmlns:ns=\"http://schemas.microsoft....\n",
      "3             274.0  <ns:Resume xmlns:ns=\"http://schemas.microsoft....\n",
      "4               NaN  <ns:Resume xmlns:ns=\"http://schemas.microsoft....\n",
      "Merged DataFrame:\n",
      "   BusinessEntityID NationalIDNumber   BirthDate MaritalStatus Gender  \\\n",
      "0                 1        295847284  1969-01-29             S      M   \n",
      "1                 2        245797967  1971-08-01             S      F   \n",
      "2                 3        509647174  1974-11-12             M      M   \n",
      "3                 4        112457891  1974-12-23             S      M   \n",
      "4                 5        695256908  1952-09-27             M      F   \n",
      "\n",
      "     HireDate  VacationHours  SickLeaveHours  \\\n",
      "0  2009-01-14             99              69   \n",
      "1  2008-01-31              1              20   \n",
      "2  2007-11-11              2              21   \n",
      "3  2007-12-05             48              80   \n",
      "4  2008-01-06              5              22   \n",
      "\n",
      "  employee_location_in_corporate_hierarchy  \\\n",
      "0                                     None   \n",
      "1                                      /1/   \n",
      "2                                    /1/1/   \n",
      "3                                  /1/1/1/   \n",
      "4                                  /1/1/2/   \n",
      "\n",
      "  employee_depth_in_corporate_hierarchy Resume  \n",
      "0                                  None    NaN  \n",
      "1                                     1    NaN  \n",
      "2                                     2    NaN  \n",
      "3                                     3    NaN  \n",
      "4                                     3    NaN  \n",
      "DELETE FROM dimension.Personal_information\n",
      "The table dimension.Personal_information is empty.\n",
      "Data successfully inserted into dimension.Personal_information\n",
      "Success email sent successfully.\n"
     ]
    }
   ],
   "source": [
    "def extract_load_personal_information():\n",
    "    try:\n",
    "        # Extract Employee information\n",
    "        df_employee_info = pd.read_sql(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                BusinessEntityID,\n",
    "                NationalIDNumber,\n",
    "                BirthDate,\n",
    "                MaritalStatus,\n",
    "                Gender,\n",
    "                HireDate,\n",
    "                VacationHours,\n",
    "                SickLeaveHours,\n",
    "                CAST(OrganizationNode AS VARCHAR(100)) AS employee_location_in_corporate_hierarchy,\n",
    "                CAST(OrganizationLevel AS VARCHAR(100)) As employee_depth_in_corporate_hierarchy\n",
    "            FROM HumanResources.Employee\n",
    "            \"\"\",\n",
    "            engine_source\n",
    "        )\n",
    "        print(\"Employee information loaded into DataFrame:\")\n",
    "        print(df_employee_info.head())\n",
    "\n",
    "        # Extract JobCandidate information\n",
    "        df_job_candidate = pd.read_sql(\n",
    "            \"\"\"\n",
    "            SELECT \n",
    "                BusinessEntityID,\n",
    "                Resume\n",
    "            FROM HumanResources.JobCandidate\n",
    "            \"\"\",\n",
    "            engine_source\n",
    "        )\n",
    "        print(\"JobCandidate information loaded into DataFrame:\")\n",
    "        print(df_job_candidate.head())\n",
    "\n",
    "        # Merge data\n",
    "        df_personal_info = pd.merge(\n",
    "            df_employee_info,\n",
    "            df_job_candidate,\n",
    "            on='BusinessEntityID',\n",
    "            how='left'\n",
    "        )\n",
    "        print(\"Merged DataFrame:\")\n",
    "        print(df_personal_info.head())\n",
    "\n",
    "        # Drop the computed column to avoid errors during insertion\n",
    "        # df_personal_info = df_personal_info.drop(columns=['employee_depth_in_corporate_hierarchy'])\n",
    "\n",
    "        # Load data to target table\n",
    "        target_schema = 'dimension'\n",
    "        target_table = 'Personal_information'\n",
    "\n",
    "        delete_table_contents(engine_target, target_schema, target_table)\n",
    "        check_table_empty(engine_target, target_schema, target_table)\n",
    "        df_personal_info = df_personal_info[['BusinessEntityID', 'NationalIDNumber', 'BirthDate', 'MaritalStatus',\n",
    "       'Gender', 'HireDate', 'VacationHours', 'SickLeaveHours','Resume',\n",
    "       'employee_location_in_corporate_hierarchy' ,'employee_depth_in_corporate_hierarchy']]\n",
    "\n",
    "        df_personal_info.to_sql(\n",
    "            name=target_table,\n",
    "            con=engine_target,\n",
    "            schema=target_schema,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            chunksize=1000\n",
    "        )\n",
    "        print(\"Data successfully inserted into dimension.Personal_information\")\n",
    "        \n",
    "        send_mail_success(\n",
    "        subject='ETL Success: Personal information Transformation',\n",
    "        message='The data transformation for the Personal information  History table was successful.'\n",
    "        )\n",
    "\n",
    "    except:\n",
    "        logger.exception(\"error test\")\n",
    "        send_mail_error(error = str (traceback.format_exc()),\n",
    "                    subject = 'Error test')\n",
    "\n",
    "\n",
    "# Run the ETL process for Personal_information table\n",
    "extract_load_personal_information()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Job (id=e50a520012c14b82ba7a3b8c7a979ba6 name=extract_load_personal_information)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.add_job(func=extract_load_personal_information, trigger=\"interval\", minutes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try:\n",
    "#     # Extract Employee information\n",
    "#     df_employee_info = pd.read_sql(\n",
    "#         \"\"\"\n",
    "#         SELECT \n",
    "#             BusinessEntityID,\n",
    "#             NationalIDNumber,\n",
    "#             BirthDate,\n",
    "#             MaritalStatus,\n",
    "#             Gender,\n",
    "#             HireDate,\n",
    "#             VacationHours,\n",
    "#             SickLeaveHours,\n",
    "#             CAST(OrganizationNode AS VARCHAR(100)) AS employee_location_in_corporate_hierarchy\n",
    "#         FROM HumanResources.Employee\n",
    "#         \"\"\",\n",
    "#         engine_source\n",
    "#     )\n",
    "#     # print(\"Employee information loaded into DataFrame:\")\n",
    "#     # print(df_employee_info.head())\n",
    "\n",
    "#     # Extract JobCandidate information\n",
    "#     df_job_candidate = pd.read_sql(\n",
    "#         \"\"\"\n",
    "#         SELECT \n",
    "#             BusinessEntityID,\n",
    "#             Resume\n",
    "#         FROM HumanResources.JobCandidate\n",
    "#         \"\"\",\n",
    "#         engine_source\n",
    "#     )\n",
    "#     #print(\"JobCandidate information loaded into DataFrame:\")\n",
    "#     #print(df_job_candidate.head())\n",
    "\n",
    "#     # Merge data\n",
    "#     df_personal_info = pd.merge(\n",
    "#         df_employee_info,\n",
    "#         df_job_candidate,\n",
    "#         on='BusinessEntityID',\n",
    "#         how='left'\n",
    "#     )\n",
    "#     #print(\"Merged DataFrame:\")\n",
    "#     print(df_personal_info.head())\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during ETL process for Personal_information: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  df_personal_info.head()\n",
    "# df_personal_info[['BusinessEntityID', 'NationalIDNumber', 'BirthDate', 'MaritalStatus',\n",
    "#        'Gender', 'HireDate', 'VacationHours', 'SickLeaveHours','Resume',\n",
    "#        'employee_location_in_corporate_hierarchy' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  schema_name, table_name in result:\n",
    "    with engine_target.connect() as connection:\n",
    "        query = f'alter table {schema_name}.{table_name} check constraint all'\n",
    "        connection.execute(text(query).execution_options(autocommit=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success email sent successfully.\n"
     ]
    }
   ],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def send_mail_success(\n",
    "    sender=\"osamamenna1511@gmail.com\",  # put sender email here \n",
    "    password='lwrx jrtd uhjm cjvt',  # use your app-specific password here\n",
    "    to_list=[\"menna1eminshawy@gmail.com\"],  # receiver list here\n",
    "    message=\"Data transformation completed successfully.\",\n",
    "    subject=\"Data Transformation Success\",\n",
    "):\n",
    "    msg = MIMEMultipart()\n",
    "    msg[\"From\"] = sender\n",
    "    msg[\"To\"] = \", \".join(to_list)\n",
    "    msg[\"Subject\"] = subject\n",
    "    html_message = f\"\"\"<html><body>{message}</body></html>\"\"\"\n",
    "    msg.attach(MIMEText(html_message, \"html\"))\n",
    "    \n",
    "    try:\n",
    "        # sends email\n",
    "        smtpserver = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
    "        smtpserver.starttls()\n",
    "        smtpserver.login(sender, password)\n",
    "        smtpserver.sendmail(msg[\"From\"], to_list, msg.as_string())\n",
    "        smtpserver.quit()\n",
    "        print(\"Success email sent successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send success email: {e}\")\n",
    "\n",
    "# Example usage after data transformation is done\n",
    "send_mail_success(\n",
    "    message=\"The data transformation process has been completed successfully and is ready for further steps.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
